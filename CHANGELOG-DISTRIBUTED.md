# Changelog - TransformaÃ§Ã£o para Sistema DistribuÃ­do

## ðŸš€ VersÃ£o 2.0.0 - Sistema DistribuÃ­do (2025-01-09)

### ðŸŽ¯ Objetivo

Transformar o sistema monolÃ­tico em um **sistema distribuÃ­do com escalabilidade horizontal**, mantendo simplicidade e estabilidade.

### âœ… MudanÃ§as Implementadas

#### 1. **Database: SQLite â†’ PostgreSQL**

**Antes:**
```python
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': BASE_DIR / 'db.sqlite3',
    }
}
```

**Depois:**
```python
DATABASES = {
    'default': dj_database_url.config(
        default='postgresql://mutiroes_user:mutiroes_pass_2024_secure@postgres:5432/mutiroes_db',
        conn_max_age=600,
        conn_health_checks=True,
    )
}
```

**Motivo:** SQLite nÃ£o suporta mÃºltiplas conexÃµes simultÃ¢neas de diferentes rÃ©plicas. PostgreSQL permite que todas as rÃ©plicas do backend compartilhem o mesmo banco.

**Impacto:**
- âœ… MÃºltiplas rÃ©plicas podem escrever/ler simultaneamente
- âœ… Connection pooling (600s max age)
- âœ… Health checks automÃ¡ticos
- âœ… Preparado para replicaÃ§Ã£o master-slave

---

#### 2. **Backend: 1 InstÃ¢ncia â†’ 3 RÃ©plicas**

**Antes:**
```yaml
backend:
  ports:
    - "8000:8000"
  command: python manage.py runserver 0.0.0.0:8000
```

**Depois:**
```yaml
backend1:
  # Sem porta exposta (Nginx faz proxy)
  command: gunicorn mutiroes_backend.wsgi:application --bind 0.0.0.0:8000 --workers 4

backend2:
  command: gunicorn mutiroes_backend.wsgi:application --bind 0.0.0.0:8000 --workers 4

backend3:
  command: gunicorn mutiroes_backend.wsgi:application --bind 0.0.0.0:8000 --workers 4
```

**Motivo:** 
- Gunicorn Ã© production-ready (vs runserver para dev)
- 3 rÃ©plicas permitem load balancing e failover
- 4 workers por rÃ©plica = 12 workers HTTP simultÃ¢neos

**Impacto:**
- âœ… Alta disponibilidade (se uma rÃ©plica cai, outras continuam)
- âœ… Escalabilidade (pode adicionar mais rÃ©plicas facilmente)
- âœ… Performance (12 workers vs 1 runserver)

---

#### 3. **Load Balancer: AdiÃ§Ã£o do Nginx**

**Antes:**
- RequisiÃ§Ãµes diretas ao backend na porta 8000
- Sem distribuiÃ§Ã£o de carga
- Single point of failure

**Depois:**
```nginx
upstream backend_servers {
    least_conn;  # Algoritmo: menos conexÃµes
    server backend1:8000 max_fails=3 fail_timeout=30s;
    server backend2:8000 max_fails=3 fail_timeout=30s;
    server backend3:8000 max_fails=3 fail_timeout=30s;
}

server {
    listen 80;
    location /api/ {
        proxy_pass http://backend_servers;
        # Rate limiting, health checks, etc
    }
}
```

**Motivo:**
- Distribuir requisiÃ§Ãµes entre backends
- Detectar backends unhealthy e redirecionar trÃ¡fego
- Rate limiting centralizado
- Cache de static/media files

**Impacto:**
- âœ… DistribuiÃ§Ã£o automÃ¡tica de carga
- âœ… Failover automÃ¡tico (backend falha â†’ redireciona)
- âœ… Rate limiting (100 req/s API, 10 req/min auth)
- âœ… ProteÃ§Ã£o contra DDoS
- âœ… Cache de arquivos estÃ¡ticos

---

#### 4. **Redis: PersistÃªncia Melhorada**

**Antes:**
```yaml
redis:
  image: redis:7-alpine
  # Sem configuraÃ§Ã£o de persistÃªncia
```

**Depois:**
```yaml
redis:
  command: redis-server --appendonly yes --appendfsync everysec --save 60 1000
```

**Motivo:** Redis usado como message broker e cache. Perder dados = perder tasks e sessions.

**Impacto:**
- âœ… AOF (Append Only File): Grava cada write no disco
- âœ… RDB (Snapshot): Snapshot a cada 60s se 1000+ mudanÃ§as
- âœ… RecuperaÃ§Ã£o automÃ¡tica apÃ³s restart
- âœ… Durabilidade de tasks e sessions

---

#### 5. **Celery: 1 Worker â†’ 2 Workers**

**Antes:**
```yaml
celery-worker:
  command: celery -A mutiroes_backend worker --concurrency=2
```

**Depois:**
```yaml
celery-worker1:
  command: celery -A mutiroes_backend worker --concurrency=4 --max-tasks-per-child=1000

celery-worker2:
  command: celery -A mutiroes_backend worker --concurrency=4 --max-tasks-per-child=1000
```

**Motivo:**
- Processar mais tasks simultaneamente
- Distribuir carga de processamento assÃ­ncrono
- Reiniciar workers apÃ³s 1000 tasks (evita memory leaks)

**Impacto:**
- âœ… 8 tasks simultÃ¢neas (vs 2 anteriormente)
- âœ… Processamento mais rÃ¡pido de emails, relatÃ³rios, etc
- âœ… EscalÃ¡vel (pode adicionar mais workers)

---

#### 6. **Volumes Compartilhados**

**Antes:**
- Volumes separados por container
- Media files nÃ£o compartilhados

**Depois:**
```yaml
volumes:
  backend_media:   # Compartilhado entre backend1, backend2, backend3
  backend_static:  # Compartilhado entre backend1, backend2, backend3
  postgres_data:   # Persistente
  redis_data:      # Persistente
```

**Motivo:** Todas as rÃ©plicas precisam acessar os mesmos arquivos de mÃ­dia e estÃ¡ticos.

**Impacto:**
- âœ… Upload de foto no backend1 â†’ visÃ­vel em backend2 e backend3
- âœ… Static files coletados uma vez, servidos por todos
- âœ… PersistÃªncia de dados mesmo apÃ³s `docker-compose down`

---

#### 7. **Makefile: Comandos para Sistema DistribuÃ­do**

**Novos comandos:**
```bash
make scale-backends N=5    # Escalar para 5 rÃ©plicas
make scale-workers N=4     # Escalar workers
make logs-backend          # Logs de TODAS as rÃ©plicas
make logs-nginx            # Logs do load balancer
make health                # Health check completo
make info                  # Uso de recursos
```

**Motivo:** Facilitar gerenciamento de mÃºltiplas rÃ©plicas.

**Impacto:**
- âœ… Escalabilidade com 1 comando
- âœ… Monitoramento facilitado
- âœ… Debugging mais rÃ¡pido

---

#### 8. **Environment Variables**

**MudanÃ§as:**
```bash
# Antes
DEBUG=True
DATABASE_URL=sqlite:///db.sqlite3
ALLOWED_HOSTS=localhost,backend

# Depois
DEBUG=False
DATABASE_URL=postgresql://mutiroes_user:mutiroes_pass_2024_secure@postgres:5432/mutiroes_db
ALLOWED_HOSTS=localhost,backend1,backend2,backend3,nginx
```

**Motivo:** ConfiguraÃ§Ã£o para produÃ§Ã£o e mÃºltiplas rÃ©plicas.

---

### ðŸ“Š ComparaÃ§Ã£o: Antes vs Depois

| Aspecto | Monolito (v1.0) | DistribuÃ­do (v2.0) |
|---------|-----------------|-------------------|
| **Backends** | 1 instÃ¢ncia | 3 rÃ©plicas + escalÃ¡vel |
| **Workers HTTP** | 1 (runserver) | 12 (3Ã—4 Gunicorn) |
| **Celery Workers** | 1 (2 concurrent) | 2 (8 concurrent) |
| **Database** | SQLite | PostgreSQL 15 |
| **Load Balancer** | âŒ Nenhum | âœ… Nginx |
| **Failover** | âŒ Nenhum | âœ… AutomÃ¡tico |
| **Rate Limiting** | âŒ Nenhum | âœ… 100 req/s |
| **Escalabilidade** | âŒ Vertical | âœ… Horizontal |
| **Containers** | 4 | 8 (expandÃ­vel) |
| **Alta Disponibilidade** | âŒ NÃ£o | âœ… Sim |

---

### ðŸŽ¯ Capacidade

**Antes (Monolito):**
- HTTP: 1 requisiÃ§Ã£o simultÃ¢nea (runserver single-threaded)
- Tasks: 2 tasks simultÃ¢neas
- Downtime se o backend cair

**Depois (DistribuÃ­do):**
- HTTP: ~12 requisiÃ§Ãµes simultÃ¢neas (3 backends Ã— 4 workers)
- Tasks: ~8 tasks simultÃ¢neas (2 workers Ã— 4 concurrency)
- Zero downtime se 1 backend cair (outros continuam)
- EscalÃ¡vel: Adicionar mais rÃ©plicas aumenta capacidade linearmente

---

### ðŸ”§ ResiliÃªncia

**Adicionado:**

1. **Health Checks AutomÃ¡ticos**
   - Nginx verifica backends a cada 30s
   - Backend unhealthy â†’ trÃ¡fego redirecionado automaticamente

2. **Automatic Retry**
   - Request falha no backend1 â†’ tenta backend2
   - AtÃ© 3 tentativas em backends diferentes

3. **Restart Policies**
   - Container falha â†’ restart automÃ¡tico
   - Sistema reiniciado â†’ containers iniciam automaticamente

4. **Circuit Breaker**
   - ProteÃ§Ã£o contra falhas em cascata
   - 5 falhas â†’ circuit open por 60s

5. **Database Connection Pooling**
   - Reusa conexÃµes por 10 minutos
   - Health check antes de usar
   - MÃ¡ximo 100 conexÃµes simultÃ¢neas

---

### ðŸ“ˆ Performance Estimada

**Throughput:**
- Monolito: ~10-50 req/s
- DistribuÃ­do: ~1000 req/s (com cache Nginx)

**LatÃªncia:**
- Local: ~50-100ms
- Com cache: ~10-20ms

**Escalabilidade:**
- 3 backends â†’ 5 backends = +66% capacidade
- 2 workers â†’ 4 workers = +100% processamento assÃ­ncrono

---

### ðŸš€ Como Migrar

**Dados Existentes (SQLite â†’ PostgreSQL):**

```bash
# 1. Backup do SQLite
python manage.py dumpdata > backup.json

# 2. Inicie PostgreSQL
make up postgres

# 3. Migre schema
make migrate

# 4. Restaure dados
docker-compose exec backend1 python manage.py loaddata backup.json
```

**Sistema Novo:**
```bash
make quickstart
```

---

### ðŸ“ Arquivos Modificados

```
âœ… docker-compose.yml          # 8 containers (vs 4)
âœ… nginx/nginx.conf             # Load balancer config
âœ… Makefile                     # Comandos distribuÃ­dos
âœ… settings.py                  # PostgreSQL + dj-database-url
âœ… requirements.txt             # + dj-database-url
âœ… ARCHITECTURE.md              # Nova documentaÃ§Ã£o
âœ… README-DISTRIBUTED.md        # Novo README
```

---

### ðŸŽ“ LiÃ§Ãµes Aprendidas

**O que funcionou bem:**
- âœ… PostgreSQL drop-in replacement para SQLite
- âœ… Nginx least_conn Ã© simples e eficaz
- âœ… Gunicorn Ã© estÃ¡vel com 4 workers
- âœ… Volumes compartilhados funcionam perfeitamente
- âœ… Health checks do Nginx sÃ£o confiÃ¡veis

**LimitaÃ§Ãµes atuais:**
- âŒ PostgreSQL Ã© single-point-of-failure (prÃ³ximo: replicaÃ§Ã£o)
- âŒ Redis sem failover (prÃ³ximo: Sentinel)
- âŒ Nginx Ã© single instance (prÃ³ximo: HA)

---

### ðŸ”® PrÃ³ximos Passos

**Curto Prazo:**
- [ ] PostgreSQL Master-Slave Replication
- [ ] Redis Sentinel (3 nodes)
- [ ] Prometheus + Grafana
- [ ] SSL/TLS

**Longo Prazo:**
- [ ] Kubernetes (quando > 10 backends)
- [ ] Service Mesh (Istio)
- [ ] OpenTelemetry (tracing)
- [ ] CDN para static files

---

### ðŸ“š ReferÃªncias

- [ARCHITECTURE.md](./ARCHITECTURE.md) - DocumentaÃ§Ã£o completa
- [README-DISTRIBUTED.md](./README-DISTRIBUTED.md) - Guia de uso
- [Makefile](./Makefile) - Comandos disponÃ­veis
- [docker-compose.yml](./docker-compose.yml) - ConfiguraÃ§Ã£o dos containers

---

**Resultado:** Sistema monolÃ­tico transformado em **sistema distribuÃ­do real** com load balancing, escalabilidade horizontal e alta disponibilidade. âœ…
